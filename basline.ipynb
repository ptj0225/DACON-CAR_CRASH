{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import util\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':380,\n",
    "    'EPOCHS':4,\n",
    "    'LEARNING_RATE':5e-5,\n",
    "    'BATCH_SIZE':12,\n",
    "    'SEED':41,\n",
    "    'OUTLIER_RATIO': 0.05\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.efficientnet_b4(pretrained=True)\n",
    "        self.backbone.classifier[-2] = nn.Dropout(0.7)\n",
    "        self.backbone.classifier[-1] = nn.Linear(1792, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_list, label_list, tfms):\n",
    "        self.path_list = path_list\n",
    "        self.label_list = label_list\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.path_list[index])\n",
    "        if self.tfms is not None:\n",
    "            image = self.tfms(image = image)['image']\n",
    "\n",
    "        if self.label_list is not None:\n",
    "            label = np.zeros(5)\n",
    "            label[int(self.label_list[index][0])] = 1\n",
    "            label[3 + int(self.label_list[index][1])] = 1\n",
    "            label = np.float32(label)\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for imgs, labels in tqdm(iter(train_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(imgs)\n",
    "            output = torch.concat([F.softmax(output[:, :3]), F.softmax(output[:, 3:])], dim=1)\n",
    "            loss = criterion(output, target=labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            if len(train_loss) >= 5:\n",
    "                print(\"\\rtrain_loss: {0}\".format(round(np.mean(train_loss[-5:]), 5)), end=\"\")\n",
    "        _val_loss, val_score = validation(model, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}]')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "            \n",
    "    return val_score\n",
    "\n",
    "def validation(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    probs_w, probs_t, labels_w, labels_t = ([], [], [], [])\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(iter(val_loader)):\n",
    "            img = img.float().to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            prob = model(img)\n",
    "            prob = torch.concat([F.softmax(input=prob[:, :3]), F.softmax(input=prob[:, 3:])], dim=1)\n",
    "            loss = criterion(F.softmax(input=prob), target=label)\n",
    "            \n",
    "            prob = prob.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            probs_w.append(np.argmax(prob[:, :3], axis=-1))\n",
    "            probs_t.append(np.argmax(prob[:, 3:], axis=-1))\n",
    "            labels_t.append(np.argmax(label[:, :3], axis=-1))\n",
    "            labels_w.append(np.argmax(label[:, 3:], axis=-1))\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    probs_w = np.concatenate(probs_w, axis=-1, dtype=int)\n",
    "    probs_t = np.concatenate(probs_t, axis=-1, dtype=int)\n",
    "    labels_w = np.concatenate(labels_w, axis=-1, dtype=int)\n",
    "    labels_t = np.concatenate(labels_t, axis=-1, dtype=int)\n",
    "    \n",
    "    val_score = np.mean([f1_score(y_pred=probs_w, y_true=labels_w, average='macro'), f1_score(y_pred=probs_t, y_true=labels_t, average='macro')])\n",
    "    val_loss = np.mean(val_loss)\n",
    "    print(np.mean(val_loss))\n",
    "    return val_loss, val_score\n",
    "\n",
    "def psudo_label(model, train_unlabeld, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    train_dataset = CustomDataset(train_unlabeld.loc[:, 'path'].values, None, tfms_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 1, shuffle=False, num_workers=0)\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(iter(train_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            output = model(imgs)\n",
    "            output = output.to('cpu').numpy()\n",
    "            pred = np.stack([np.argmax(output[:, :3], axis=1),\n",
    "                             np.argmax(output[:, 3:], axis=1)], axis = 1)\n",
    "            preds += list(pred)\n",
    "    return np.array(preds, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_pic.csv', index_col=0)\n",
    "df = df.loc[:, ['path', 'weather', 'timing']]\n",
    "df_labeled = df[df.loc[:, 'weather'] != -1]\n",
    "\n",
    "train_unlabeld = df[df.loc[:, 'weather'] == -1]\n",
    "train_labeled = df_labeled[: int(len(df_labeled) * 0.8)]\n",
    "valid = df_labeled[int(len(df_labeled) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_train = A.Compose([A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "                        A.HorizontalFlip(p=0.5),\n",
    "                        A.ColorJitter(p=0.5),\n",
    "                        A.Rotate((-30,30), p=0.5),\n",
    "                        A.Normalize(mean=(0.3192, 0.3201, 0.3083), std=(0.2132, 0.2072, 0.2059)),\n",
    "                        ToTensorV2()])\n",
    "\n",
    "tfms_test = A.Compose([A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "                        A.Normalize(mean=(0.3192, 0.3201, 0.3083), std=(0.2132, 0.2072, 0.2059)),\n",
    "                        ToTensorV2()])\n",
    "\n",
    "valid_dataset = util.CustomDataset(valid['path'].values, valid.loc[:, ['weather', 'timing']].values, tfms_test)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8839f21c304969a44574a3d62903d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.65856"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd1678b0098424d849837bac632b0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7672266066074371\n",
      "Train Loss : [0.66491] Val Loss : [0.76723]\n",
      "epoch: 1 best val score: 0\n",
      "detecting outliers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285cbf8b142e4887a4d9829b1347b41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 data removed from 80\n",
      "psudo labeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e106319a1388496b97466d3932151e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72ad3a8930d4d8f9a9e1f78ed40a8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.60275"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8122234f3884c1d94333211f950f387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7574914693832397\n",
      "Train Loss : [0.60709] Val Loss : [0.75749]\n",
      "epoch: 2 best val score: 1.0\n",
      "detecting outliers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644a0a602aa2473e98d3a38489bd496a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 data removed from 80\n",
      "psudo labeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b7d51b087549d6a2ab460eed673334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbd6a5f1d254924ac60fb51d1ee56bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.52664"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4cd2e72d24460b8508b9a2df538d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7356239855289459\n",
      "Train Loss : [0.53047] Val Loss : [0.73562]\n",
      "epoch: 3 best val score: 1.0\n",
      "detecting outliers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab91fd732d364f1c9c6f72ff43f3df9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 data removed from 80\n",
      "psudo labeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f427c16ba1554de2815fb32a430d23c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929aa9ec6d4e4d04a5a71b2e0f194e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.45157"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dce29384734266ad4cd264a8cacd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7096829414367676\n",
      "Train Loss : [0.46445] Val Loss : [0.70968]\n",
      "epoch: 4 best val score: 1.0\n",
      "detecting outliers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e601a8fb4d7c4cedb364641b3821c8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "best_val_score = 0\n",
    "\n",
    "train_dataset = util.CustomDataset(train_labeled['path'].values, train_labeled.loc[:, ['weather', 'timing']].values, tfms_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=4)\n",
    "train(model=model, optimizer=optimizer, epochs=1, train_loader=train_loader, val_loader=valid_loader, scheduler=scheduler, device=device)\n",
    "\n",
    "for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "    print('epoch:', epoch, 'best val score:',best_val_score)\n",
    "    #labeled data 이상치 제거\n",
    "    print(\"detecting outliers...\")\n",
    "    train_labeled_updated , train_labeled_outlier = detect_outliers(model, train_labeled, outlier_ratio = CFG['OUTLIER_RATIO'], device=device)\n",
    "    print(len(train_labeled_outlier), 'data removed from', len(train_labeled))\n",
    "\n",
    "    #psudo labeling\n",
    "    print(\"psudo labeling...\")\n",
    "    psudo_labeled = psudo_label(model, pd.concat([train_labeled_outlier, train_unlabeld]), device)\n",
    "\n",
    "    # create train loader\n",
    "    train_path = np.concatenate([train_labeled_outlier['path'].values, train_unlabeld['path'].values, train_labeled_updated['path'].values])\n",
    "    train_labels = np.concatenate([psudo_labeled, train_labeled_updated.loc[:, ['weather', 'timing']].values])\n",
    "    train_dataset = util.CustomDataset(train_path, train_labels, tfms_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=4)\n",
    "\n",
    "    # train\n",
    "    print(\"training data...\")\n",
    "    val_score = train(model=model, optimizer=optimizer, epochs=1, train_loader=train_loader, val_loader=valid_loader, scheduler=scheduler, device=device)\n",
    "    if best_val_score < val_score:\n",
    "        best_val_score = val_score\n",
    "        torch.save(model.state_dict(), 'model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2076a387d2eeee7d098e4cccefb973249bebb4fa144919220ce31f67b8bf2cb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
